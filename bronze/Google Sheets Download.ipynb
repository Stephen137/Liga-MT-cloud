{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f2feadf5-d208-40e5-8480-737184b482f4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The raw data for Gdansk has been downloaded from Google Sheets and written as a CSV to Amazon S3 storage\nThe raw data for Warsaw has been downloaded from Google Sheets and written as a CSV to Amazon S3 storage\nThe raw data for Wroclaw has been downloaded from Google Sheets and written as a CSV to Amazon S3 storage\nThe raw data for Krakow has been downloaded from Google Sheets and written as a CSV to Amazon S3 storage\nThe raw data for Poznan has been downloaded from Google Sheets and written as a CSV to Amazon S3 storage\nThe raw data for Slask has been downloaded from Google Sheets and written as a CSV to Amazon S3 storage\nAll raw city data has been downloaded from Google Sheets and written as a CSV to Amazon S3 storage.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def download_google_sheets_data(city):\n",
    "\n",
    "    # Google Sheet URLs\n",
    "    city_urls = {\n",
    "        \"gdansk\": \"1EEMR48O2JlyPd3ZjJFnL1SWcnSK4Ait0m3uNrBx9vjo\",\n",
    "        \"warsaw\": \"1vEThuBu_0Wd2HiJwh7h1nyiFmjAL280ePZbZEcsIZgM\",\n",
    "        \"wroclaw\": \"1KUIqvcKVvebcanrz68wwT2-wpUaBEjlL4Arni2i1oSs\",\n",
    "        \"krakow\": \"1PlA4FRYJC4NSyO8bZXin8DaKgwua8K7ruYV_LBznmis\",\n",
    "        \"poznan\": \"1JjHTxYtK1hfu2_2vAQH2u9SrpMzgDAw_GsWQl0f1J0s\",\n",
    "        \"slask\": \"1KgNyrIiz4RNDBN7qZZOLTlBJjT5NC7H8Q3qPwPjuilI\"\n",
    "    }\n",
    "\n",
    "    gid = \"422946955\"\n",
    "    url = f\"https://docs.google.com/spreadsheets/d/{city_urls[city]}/export?format=csv&gid={gid}\"\n",
    "\n",
    "    # Read CSV into Pandas DataFrame\n",
    "    raw_df = pd.read_csv(url, header=None)    \n",
    "    spark_raw_df = spark.createDataFrame(raw_df)  \n",
    "\n",
    "    s3_output_path = f\"s3://databricks-workspace-liga-mt-bucket/unity-catalog/raw/{city}/\"\n",
    "\n",
    "    spark_raw_df.write \\\n",
    "        .format(\"csv\") \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .save(s3_output_path)    \n",
    "\n",
    "    print(f\"The raw data for {city.capitalize()} has been downloaded from Google Sheets and written as a CSV to Amazon S3 storage\")\n",
    "        \n",
    "cities = [\"gdansk\", \"warsaw\", \"wroclaw\", \"krakow\", \"poznan\", \"slask\"]\n",
    "\n",
    "# Process all cities\n",
    "for city in cities:\n",
    "    download_google_sheets_data(city)\n",
    "\n",
    "print(\"All raw city data has been downloaded from Google Sheets and written as a CSV to Amazon S3 storage.\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 3168415300539443,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Google Sheets Download",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}