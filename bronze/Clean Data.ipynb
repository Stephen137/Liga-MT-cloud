{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f8188bf9-87f8-44b6-be53-394d525c0676",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No missing results found for Gdansk. Proceeding with the next steps...\nThe Gdansk data includes a total of 1500 records.\nThe Gdansk data has been cleaned and written in parquet format to Amazon S3 storage.\nNo missing results found for Warsaw. Proceeding with the next steps...\nThe Warsaw data includes a total of 990 records.\nThe Warsaw data has been cleaned and written in parquet format to Amazon S3 storage.\nThe Wroclaw data includes a total of 749 records. There appears to be some missing or duplicate records. Please refer to the missing results file.\nThe Wroclaw data has been cleaned and written in parquet format to Amazon S3 storage.\nThe Krakow data includes a total of 653 records. There appears to be some missing or duplicate records. Please refer to the missing results file.\nThe Krakow data has been cleaned and written in parquet format to Amazon S3 storage.\nNo missing results found for Poznan. Proceeding with the next steps...\nThe Poznan data includes a total of 300 records.\nThe Poznan data has been cleaned and written in parquet format to Amazon S3 storage.\nThe Slask data includes a total of 239 records. There appears to be some missing or duplicate records. Please refer to the missing results file.\nThe Slask data has been cleaned and written in parquet format to Amazon S3 storage.\nAll city data has been cleaned and written in parquet format to Amazon S3 storage.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, FloatType, BooleanType\n",
    "\n",
    "### Clean the data\n",
    "def clean_data(city):\n",
    "\n",
    "    s3_path = f\"s3://databricks-workspace-liga-mt-bucket/unity-catalog/raw/{city}/*.csv\"\n",
    "\n",
    "    df = spark.read \\\n",
    "        .format(\"csv\") \\\n",
    "        .option(\"header\", \"false\") \\\n",
    "        .option(\"inferSchema\", \"false\") \\\n",
    "        .load(s3_path)\n",
    "\n",
    "    # Convert to pandas DataFrame for transformations\n",
    "    raw_df = df.toPandas() \n",
    "    \n",
    "    # Initialize an empty list to store the transformed data\n",
    "    transformed_data = []\n",
    "\n",
    "    # Define the columns where \"czas\" appears\n",
    "    czas_columns = [1, 8, 15, 22, 29, 36]\n",
    "\n",
    "    # Iterate through the rows to extract headers and match data\n",
    "    for i in range(len(raw_df)-1, -1, -1):\n",
    "        # Search for \"KOLEJKA\" in any column (case-insensitive and flexible regex)\n",
    "        for col in range(len(raw_df.columns)):\n",
    "            cell_value = str(raw_df.iloc[i, col]).strip().lower()  # Normalize whitespace and casing\n",
    "            if re.search(r'kolejka\\s*\\d+', cell_value):  # Flexible regex for \"KOLEJKA\" followed by optional spaces and digits\n",
    "                round_value = raw_df.iloc[i, col]  # KOLEJKA is in the current column\n",
    "                #print(f\"Found {round_value} at row {i}, column {col}\")  # Debugging output\n",
    "\n",
    "                # Search for \"czas\" in the predefined columns (1, 8, 15, 22)\n",
    "                for czas_col in czas_columns:\n",
    "                    if czas_col < len(raw_df.columns):  # Ensure column exists\n",
    "                        for j in range(i + 1, min(i + 10, len(raw_df))):  # Search within the next 10 rows\n",
    "                            if str(raw_df.iloc[j, czas_col]).strip().lower() == 'czas':\n",
    "                                #print(f\"Found 'czas' at row {j}, column {czas_col}\")  # Debugging output\n",
    "\n",
    "                                # Extract \"KATEGORIA\" and \"date\" from the rows above \"czas\"\n",
    "                                category_value = raw_df.iloc[j - 1, czas_col]  # KATEGORIA is one row above \"czas\"\n",
    "                                date_value = raw_df.iloc[j - 2, czas_col]  # Date is two rows above \"czas\"\n",
    "                                #print(f\"Found KATEGORIA: {category_value}, Date: {date_value} at row {j - 1}, column {czas_col}\")  # Debugging output\n",
    "\n",
    "                                # Iterate through the rows below \"czas\" to extract match data\n",
    "                                for k in range(j + 1, len(raw_df)):\n",
    "                                    # Skip rows with the \"czas\" header\n",
    "                                    if str(raw_df.iloc[k, czas_col]).strip().lower() == 'czas':\n",
    "                                        break  # Stop processing this column range if we encounter another \"czas\"\n",
    "\n",
    "                                    # Check if the row has data in the current column range\n",
    "                                    if pd.notna(raw_df.iloc[k, czas_col]):\n",
    "                                        match_data = {\n",
    "                                            'match_id': len(transformed_data) + 1,\n",
    "                                            'city': city,\n",
    "                                            'round': int(round_value.split()[1]),  # Extract round number\n",
    "                                            'date': date_value,\n",
    "                                            'category': category_value.replace(\"KATEGORIA\", \"\").strip().replace(\" \", \"\"),  # Clean category\n",
    "                                            'group': raw_df.iloc[k, czas_col + 5],  # Group is the sixth column\n",
    "                                            'pitch': raw_df.iloc[k, czas_col + 6],  # Pitch is the seventh column\n",
    "                                            'time': raw_df.iloc[k, czas_col],  # Time is the first column in the block\n",
    "                                            'home_team': raw_df.iloc[k, czas_col + 1].rstrip(),  # Home team is the second column\n",
    "                                            'away_team': raw_df.iloc[k, czas_col + 2].rstrip(),  # Away team is the third column\n",
    "                                            'home_goals': raw_df.iloc[k, czas_col + 3],  # Home goals is the fourth column\n",
    "                                            'away_goals': raw_df.iloc[k, czas_col + 4],  # Away goals is the fifth column                                       \n",
    "                                        }\n",
    "                                        transformed_data.append(match_data)\n",
    "                                    else:\n",
    "                                        # Stop processing this column range if we encounter an empty row\n",
    "                                        break\n",
    "                                break  # Stop searching for \"czas\" once found\n",
    "\n",
    "    # Convert the list to a DataFrame\n",
    "    transformed_data_df = pd.DataFrame(transformed_data)\n",
    "\n",
    "    current_year = 2025\n",
    "    \n",
    "    def parse_date(date_str):\n",
    "        # Try extracting various date formats\n",
    "        match = re.search(r'(\\d{4})-(\\d{2})-(\\d{2})', date_str)  # Match YYYY-MM-DD\n",
    "        if match:\n",
    "            return f\"{match.group(3)}/{match.group(2)}/{match.group(1)}\"  # Convert to DD/MM/YYYY\n",
    "\n",
    "        match = re.search(r'-(\\s*\\d{1,2}\\.\\d{1,2})\\s*/', date_str)  # Match -DD.MM/\n",
    "        if match:\n",
    "            date_part = match.group(1).strip()\n",
    "        else:\n",
    "            match = re.search(r'-(.*)', date_str)  # Match -<everything after>\n",
    "            if match:\n",
    "                date_part = match.group(1).strip()\n",
    "            else:\n",
    "                match = re.search(r'\\b\\w+\\s+(\\d{1,2}\\.\\d{1,2})', date_str)  # Match \"Niedziela DD.MM\"\n",
    "                if match:\n",
    "                    date_part = match.group(1)\n",
    "                else:\n",
    "                    return None  # No valid date found\n",
    "\n",
    "        # Convert to datetime format if it's a valid date\n",
    "        try:\n",
    "            return pd.to_datetime(f\"{date_part}.{current_year}\", format='%d.%m.%Y').strftime('%d/%m/%Y')\n",
    "        except ValueError:\n",
    "            return date_part  # If it's not a date, return raw text\n",
    "\n",
    "    # Apply the function to the date_string column and create a new ingestion_date column\n",
    "    transformed_data_df['date'] = transformed_data_df['date'].apply(parse_date)\n",
    "\n",
    "    transformed_data_df[\"missing_result_flag\"] = (\n",
    "        transformed_data_df[\"home_goals\"].isna() | \n",
    "        transformed_data_df[\"away_goals\"].isna() | \n",
    "        (transformed_data_df[\"home_goals\"] == \"x\") | \n",
    "        (transformed_data_df[\"away_goals\"] == \"x\")    \n",
    "    ).astype(bool)\n",
    "\n",
    "    # Filter rows where missing_result_flag is True\n",
    "    missing_results_df = transformed_data_df[transformed_data_df[\"missing_result_flag\"]].copy()    \n",
    "    \n",
    "    # Define schema based on transformed_data_df structure\n",
    "    schema = StructType([\n",
    "        StructField(\"match_id\", IntegerType(), True),\n",
    "        StructField(\"city\", StringType(), True),\n",
    "        StructField(\"round\", IntegerType(), True),\n",
    "        StructField(\"date\", StringType(), True),\n",
    "        StructField(\"category\", StringType(), True),\n",
    "        StructField(\"group\", StringType(), True),\n",
    "        StructField(\"pitch\", IntegerType(), True),\n",
    "        StructField(\"time\", StringType(), True),\n",
    "        StructField(\"home_team\", StringType(), True),\n",
    "        StructField(\"away_team\", StringType(), True),\n",
    "        StructField(\"home_goals\", IntegerType(), True),\n",
    "        StructField(\"away_goals\", FloatType(), True),\n",
    "        StructField(\"missing_result_flag\", BooleanType(), True),\n",
    "    ])\n",
    "\n",
    "\n",
    "    # Ensure `home_goals` and `away_goals` are of proper type in Pandas before conversion\n",
    "    missing_results_df[\"home_goals\"] = missing_results_df[\"home_goals\"].astype(str).replace(\"nan\", None)\n",
    "    missing_results_df[\"away_goals\"] = missing_results_df[\"away_goals\"].astype(str).replace(\"nan\", None)\n",
    "\n",
    "    # Convert to Spark DataFrame only if there is data\n",
    "    if not missing_results_df.empty:\n",
    "        missing_results_spark_df = spark.createDataFrame(missing_results_df)       \n",
    "    else:\n",
    "        # Create an empty Spark DataFrame with the defined schema\n",
    "        missing_results_spark_df = spark.createDataFrame([], schema)\n",
    "        print(f\"No missing results found for {city.capitalize()}. Proceeding with the next steps...\")   \n",
    "\n",
    "\n",
    "    # Write missing results report as a parquet to the bronze layer\n",
    "    missing_results_output_path = f\"s3://databricks-workspace-liga-mt-bucket/unity-catalog/bronze/{city}\"\n",
    "    missing_results_spark_df.write \\\n",
    "        .format(\"parquet\") \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .save(missing_results_output_path) \n",
    "\n",
    "    # Filter out missing results and clean the data\n",
    "    df_bronze_data = transformed_data_df[~transformed_data_df[\"missing_result_flag\"]].drop(columns=[\"missing_result_flag\"])\n",
    "\n",
    "    # Cast to integers\n",
    "    df_bronze_data[\"ingestion_date\"] = pd.Timestamp.now()\n",
    "\n",
    "    if len(df_bronze_data) % 6 == 0:\n",
    "        print(f\"The {city.capitalize()} data includes a total of {len(df_bronze_data)} records.\")\n",
    "    else:\n",
    "        print(f\"The {city.capitalize()} data includes a total of {len(df_bronze_data)} records. There appears to be some missing or duplicate records. Please refer to the missing results file.\")\n",
    "\n",
    "    s3_output_path = f\"s3://databricks-workspace-liga-mt-bucket/unity-catalog/bronze/{city}/\"\n",
    "\n",
    "        # Save the transformed data to parquet in s3\n",
    "    spark_df_bronze_data = spark.createDataFrame(df_bronze_data)\n",
    "    spark_df_bronze_data.write \\\n",
    "        .format(\"parquet\") \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .save(s3_output_path)   \n",
    "\n",
    "    print(f\"The {city.capitalize()} data has been cleaned and written in parquet format to Amazon S3 storage.\")  \n",
    "\n",
    "cities = [\"gdansk\", \"warsaw\", \"wroclaw\", \"krakow\", \"poznan\", \"slask\"]\n",
    "\n",
    "# Process all cities\n",
    "for city in cities:\n",
    "    clean_data(city)\n",
    "\n",
    "print(\"All city data has been cleaned and written in parquet format to Amazon S3 storage.\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 3168415300539449,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Clean Data",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}